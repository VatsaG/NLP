{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "WordLevelEngMarNMT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vn-bUQJrlIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb152fa8-5c18-40ac-8fef-17d341b0714b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auS4owESrlIq",
        "colab_type": "code",
        "outputId": "389ebcc4-2b5f-4565-cc25-56beac7eb4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "lines= pd.read_table('san.txt', names=['eng', 'san'])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rco4poZrlIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lowercase all characters\n",
        "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
        "lines.san=lines.san.apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZWG_L3wrlIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove quotes\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.san=lines.san.apply(lambda x: re.sub(\"'\", '', x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRIVlxnqrlI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "# Remove all the special characters\n",
        "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines.san=lines.san.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWdyfueMrlI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove all numbers from text\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "lines.san = lines.san.apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1jMrzk5rlI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove extra spaces\n",
        "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
        "lines.san=lines.san.apply(lambda x: x.strip())\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.san=lines.san.apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPZQpxNrlI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add start and end tokens to target sequences\n",
        "lines.eng = lines.eng.apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao55eS8krlJE",
        "colab_type": "code",
        "outputId": "18608d1c-054e-442a-a5f0-029c6fe62864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "lines.sample(10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>san</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17910</th>\n",
              "      <td>START_ is that black bag yours _END</td>\n",
              "      <td>ती काळी बॅग तुमची आहे का</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32709</th>\n",
              "      <td>START_ australia is smaller than south america...</td>\n",
              "      <td>ऑस्ट्रेलिया दक्षिण अमेरिकेपेक्षा लहान आहे</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>START_ wait here _END</td>\n",
              "      <td>इथे थांबा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2696</th>\n",
              "      <td>START_ give me my bag _END</td>\n",
              "      <td>मला माझी पिशवी द्या</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7048</th>\n",
              "      <td>START_ is that your room _END</td>\n",
              "      <td>ती तुझी खोली आहे का</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29978</th>\n",
              "      <td>START_ we have five fingers on each hand _END</td>\n",
              "      <td>आपल्याला प्रत्येक हातात पाच बोटं असतात</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14944</th>\n",
              "      <td>START_ this bag is too heavy _END</td>\n",
              "      <td>ही बॅग खूपच जड आहे</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35808</th>\n",
              "      <td>START_ i never for a moment imagined that i wo...</td>\n",
              "      <td>मी या वयातही असलं काहीतरी करत असेन असा एका क्ष...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32540</th>\n",
              "      <td>START_ the only thing i have now are memories ...</td>\n",
              "      <td>माझ्याकडे आता काय उरलंय तर फक्त आठवणी</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17305</th>\n",
              "      <td>START_ did you call the police _END</td>\n",
              "      <td>तुम्ही पोलिसांना फोन केलात का</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng                                                san\n",
              "17910                START_ is that black bag yours _END                           ती काळी बॅग तुमची आहे का\n",
              "32709  START_ australia is smaller than south america...          ऑस्ट्रेलिया दक्षिण अमेरिकेपेक्षा लहान आहे\n",
              "416                                START_ wait here _END                                          इथे थांबा\n",
              "2696                          START_ give me my bag _END                                मला माझी पिशवी द्या\n",
              "7048                       START_ is that your room _END                                ती तुझी खोली आहे का\n",
              "29978      START_ we have five fingers on each hand _END             आपल्याला प्रत्येक हातात पाच बोटं असतात\n",
              "14944                  START_ this bag is too heavy _END                                 ही बॅग खूपच जड आहे\n",
              "35808  START_ i never for a moment imagined that i wo...  मी या वयातही असलं काहीतरी करत असेन असा एका क्ष...\n",
              "32540  START_ the only thing i have now are memories ...              माझ्याकडे आता काय उरलंय तर फक्त आठवणी\n",
              "17305                START_ did you call the police _END                      तुम्ही पोलिसांना फोन केलात का"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8qGpbyhrlJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_san_words=set()\n",
        "for san in lines.san:\n",
        "    for word in san.split():\n",
        "        if word not in all_san_words:\n",
        "            all_san_words.add(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GrsSUgsrlJS",
        "colab_type": "code",
        "outputId": "df428919-1a95-40ff-ba82-c24fcf36492e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Max Length of source sequence\n",
        "lenght_list=[]\n",
        "for l in lines.san:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_src = np.max(lenght_list)\n",
        "max_length_src"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0AtzE2UrlJZ",
        "colab_type": "code",
        "outputId": "b266b5cb-b2a4-4fd6-9885-1a68297e443b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Max Length of target sequence\n",
        "lenght_list=[]\n",
        "for l in lines.eng:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_tar = np.max(lenght_list)\n",
        "max_length_tar"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ltNET9rlJe",
        "colab_type": "code",
        "outputId": "75010904-6663-459b-ea5e-2ff93db30994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "target_words = sorted(list(all_eng_words))\n",
        "input_words = sorted(list(all_san_words))\n",
        "num_decoder_tokens = len(all_eng_words)\n",
        "num_encoder_tokens = len(all_san_words)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13007, 5473)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PF18ZgvrlJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_decoder_tokens += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmQEhsoKrlJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_f1yUK0rlJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lY54SjOrlJq",
        "colab_type": "code",
        "outputId": "a3efc4c0-0756-458e-a684-0a560f491667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "lines = shuffle(lines)\n",
        "lines.head(10)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>san</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35794</th>\n",
              "      <td>START_ if cleopatras nose had been shorter the...</td>\n",
              "      <td>जर क्लिओपात्राचं नाक थोडं छोटं असतं तर जगाचा इ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18786</th>\n",
              "      <td>START_ youre breaking the law _END</td>\n",
              "      <td>तू कायदा मोडत आहेस</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23339</th>\n",
              "      <td>START_ your chicken soup is great _END</td>\n",
              "      <td>तुझं चिकन सूप मस्त असतं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14534</th>\n",
              "      <td>START_ im reading this book _END</td>\n",
              "      <td>मी हे पुस्तक वाचतोय</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20823</th>\n",
              "      <td>START_ i get up at six every day _END</td>\n",
              "      <td>मी रोज सहा वाजता उठतो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33521</th>\n",
              "      <td>START_ if you dont hurry youll miss the train ...</td>\n",
              "      <td>घाई केली नाहीत तर तमची ट्रेन सुटेल</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29281</th>\n",
              "      <td>START_ what happened wasnt toms fault _END</td>\n",
              "      <td>जे काही घडलं त्यात टॉमची चूक नव्हती</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23460</th>\n",
              "      <td>START_ do you understand this book _END</td>\n",
              "      <td>हे पुस्तक तुला समजलं का</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32795</th>\n",
              "      <td>START_ i dont want to translate that sentence ...</td>\n",
              "      <td>मला त्या वाक्याचा अनुवाद करायचा नाहीये</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6358</th>\n",
              "      <td>START_ are you forgetful _END</td>\n",
              "      <td>तू विसराळू आहेस का</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     eng                                                san\n",
              "35794  START_ if cleopatras nose had been shorter the...  जर क्लिओपात्राचं नाक थोडं छोटं असतं तर जगाचा इ...\n",
              "18786                 START_ youre breaking the law _END                                 तू कायदा मोडत आहेस\n",
              "23339             START_ your chicken soup is great _END                            तुझं चिकन सूप मस्त असतं\n",
              "14534                   START_ im reading this book _END                                मी हे पुस्तक वाचतोय\n",
              "20823              START_ i get up at six every day _END                              मी रोज सहा वाजता उठतो\n",
              "33521  START_ if you dont hurry youll miss the train ...                 घाई केली नाहीत तर तमची ट्रेन सुटेल\n",
              "29281         START_ what happened wasnt toms fault _END                जे काही घडलं त्यात टॉमची चूक नव्हती\n",
              "23460            START_ do you understand this book _END                            हे पुस्तक तुला समजलं का\n",
              "32795  START_ i dont want to translate that sentence ...             मला त्या वाक्याचा अनुवाद करायचा नाहीये\n",
              "6358                       START_ are you forgetful _END                                 तू विसराळू आहेस का"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJdRoAUirlJt",
        "colab_type": "code",
        "outputId": "ee8f423d-9cea-456e-93ba-4f249c032a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Train - Test Split\n",
        "X, y =  lines.san, lines.eng\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32248,), (3584,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_-S3yTerlJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Save the train and test dataframes\n",
        "X_train.to_pickle('X_train.pkl')\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzxIVRRqrlJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkqbudVErlJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhjwQ4t4rlJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d5ccd2f7-2311-466f-f33d-8609075dbe33"
      },
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 05:33:33.346143 139929746868096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0827 05:33:33.384639 139929746868096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0827 05:33:33.394333 139929746868096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0827 05:33:33.624025 139929746868096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZFuo4xmrlKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keylG-corlKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "e1db6a13-b46c-420a-8ab0-1be15fc43fc2"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0827 05:33:46.247920 139929746868096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0827 05:33:46.271605 139929746868096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbA6ewqUrlKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 420"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z14zp1scrlKU",
        "colab_type": "code",
        "outputId": "fc40c72a-5d47-4481-9b18-6f7c05966c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 4.6997 - acc: 0.2380 - val_loss: 4.6325 - val_acc: 0.2484\n",
            "Epoch 2/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 4.4461 - acc: 0.2625 - val_loss: 4.4108 - val_acc: 0.2699\n",
            "Epoch 3/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 4.2156 - acc: 0.2879 - val_loss: 4.2185 - val_acc: 0.2956\n",
            "Epoch 4/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 3.9982 - acc: 0.3201 - val_loss: 4.0215 - val_acc: 0.3262\n",
            "Epoch 5/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 3.7923 - acc: 0.3500 - val_loss: 3.8413 - val_acc: 0.3525\n",
            "Epoch 6/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 3.6139 - acc: 0.3745 - val_loss: 3.6972 - val_acc: 0.3707\n",
            "Epoch 7/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 3.4603 - acc: 0.3942 - val_loss: 3.5804 - val_acc: 0.3862\n",
            "Epoch 8/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 3.3301 - acc: 0.4115 - val_loss: 3.4832 - val_acc: 0.4013\n",
            "Epoch 9/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 3.2156 - acc: 0.4271 - val_loss: 3.4037 - val_acc: 0.4127\n",
            "Epoch 10/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 3.1113 - acc: 0.4428 - val_loss: 3.3300 - val_acc: 0.4264\n",
            "Epoch 11/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 3.0177 - acc: 0.4574 - val_loss: 3.2821 - val_acc: 0.4386\n",
            "Epoch 12/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 2.9315 - acc: 0.4704 - val_loss: 3.2049 - val_acc: 0.4478\n",
            "Epoch 13/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 2.8483 - acc: 0.4830 - val_loss: 3.1554 - val_acc: 0.4554\n",
            "Epoch 14/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 2.7728 - acc: 0.4953 - val_loss: 3.1113 - val_acc: 0.4617\n",
            "Epoch 15/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 2.6985 - acc: 0.5074 - val_loss: 3.0582 - val_acc: 0.4709\n",
            "Epoch 16/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 2.6264 - acc: 0.5191 - val_loss: 3.0227 - val_acc: 0.4768\n",
            "Epoch 17/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 2.5570 - acc: 0.5305 - val_loss: 2.9652 - val_acc: 0.4853\n",
            "Epoch 18/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 2.4905 - acc: 0.5417 - val_loss: 2.9249 - val_acc: 0.4910\n",
            "Epoch 19/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 2.4292 - acc: 0.5519 - val_loss: 2.8904 - val_acc: 0.4969\n",
            "Epoch 20/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 2.3705 - acc: 0.5628 - val_loss: 2.8542 - val_acc: 0.5046\n",
            "Epoch 21/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 2.3121 - acc: 0.5731 - val_loss: 2.8250 - val_acc: 0.5098\n",
            "Epoch 22/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 2.2576 - acc: 0.5831 - val_loss: 2.7893 - val_acc: 0.5168\n",
            "Epoch 23/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 2.2074 - acc: 0.5927 - val_loss: 2.7599 - val_acc: 0.5222\n",
            "Epoch 24/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 2.1586 - acc: 0.6021 - val_loss: 2.7397 - val_acc: 0.5288\n",
            "Epoch 25/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 2.1131 - acc: 0.6108 - val_loss: 2.7142 - val_acc: 0.5342\n",
            "Epoch 26/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 2.0689 - acc: 0.6191 - val_loss: 2.7025 - val_acc: 0.5365\n",
            "Epoch 27/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 2.0270 - acc: 0.6277 - val_loss: 2.6812 - val_acc: 0.5390\n",
            "Epoch 28/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.9870 - acc: 0.6362 - val_loss: 2.6568 - val_acc: 0.5445\n",
            "Epoch 29/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 1.9507 - acc: 0.6443 - val_loss: 2.6365 - val_acc: 0.5500\n",
            "Epoch 30/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.9151 - acc: 0.6515 - val_loss: 2.6302 - val_acc: 0.5508\n",
            "Epoch 31/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 1.8810 - acc: 0.6593 - val_loss: 2.6164 - val_acc: 0.5556\n",
            "Epoch 32/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 1.8492 - acc: 0.6663 - val_loss: 2.6062 - val_acc: 0.5571\n",
            "Epoch 33/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 1.8173 - acc: 0.6734 - val_loss: 2.5868 - val_acc: 0.5634\n",
            "Epoch 34/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 1.7865 - acc: 0.6798 - val_loss: 2.5831 - val_acc: 0.5638\n",
            "Epoch 35/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 1.7582 - acc: 0.6856 - val_loss: 2.5620 - val_acc: 0.5704\n",
            "Epoch 36/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.7279 - acc: 0.6919 - val_loss: 2.5497 - val_acc: 0.5732\n",
            "Epoch 37/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 1.7007 - acc: 0.6983 - val_loss: 2.5381 - val_acc: 0.5754\n",
            "Epoch 38/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 1.6753 - acc: 0.7030 - val_loss: 2.5333 - val_acc: 0.5780\n",
            "Epoch 39/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.6483 - acc: 0.7086 - val_loss: 2.5223 - val_acc: 0.5789\n",
            "Epoch 40/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 1.6227 - acc: 0.7135 - val_loss: 2.5132 - val_acc: 0.5820\n",
            "Epoch 41/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 1.5987 - acc: 0.7193 - val_loss: 2.5073 - val_acc: 0.5829\n",
            "Epoch 42/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 1.5730 - acc: 0.7241 - val_loss: 2.4992 - val_acc: 0.5843\n",
            "Epoch 43/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 1.5476 - acc: 0.7286 - val_loss: 2.4969 - val_acc: 0.5853\n",
            "Epoch 44/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.5233 - acc: 0.7324 - val_loss: 2.4856 - val_acc: 0.5884\n",
            "Epoch 45/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 1.4999 - acc: 0.7373 - val_loss: 2.4749 - val_acc: 0.5917\n",
            "Epoch 46/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.4770 - acc: 0.7412 - val_loss: 2.4686 - val_acc: 0.5931\n",
            "Epoch 47/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 1.4541 - acc: 0.7460 - val_loss: 2.4619 - val_acc: 0.5941\n",
            "Epoch 48/420\n",
            "251/251 [==============================] - 65s 260ms/step - loss: 1.4329 - acc: 0.7491 - val_loss: 2.4609 - val_acc: 0.5934\n",
            "Epoch 49/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.4114 - acc: 0.7537 - val_loss: 2.4488 - val_acc: 0.5959\n",
            "Epoch 50/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 1.3931 - acc: 0.7569 - val_loss: 2.4422 - val_acc: 0.5980\n",
            "Epoch 51/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.3739 - acc: 0.7610 - val_loss: 2.4412 - val_acc: 0.5988\n",
            "Epoch 52/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.3559 - acc: 0.7646 - val_loss: 2.4398 - val_acc: 0.5995\n",
            "Epoch 53/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.3395 - acc: 0.7675 - val_loss: 2.4400 - val_acc: 0.5991\n",
            "Epoch 54/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 1.3217 - acc: 0.7714 - val_loss: 2.4345 - val_acc: 0.6018\n",
            "Epoch 55/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.3071 - acc: 0.7741 - val_loss: 2.4449 - val_acc: 0.5999\n",
            "Epoch 56/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 1.2924 - acc: 0.7770 - val_loss: 2.4305 - val_acc: 0.6022\n",
            "Epoch 57/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.2776 - acc: 0.7798 - val_loss: 2.4296 - val_acc: 0.6032\n",
            "Epoch 58/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 1.2642 - acc: 0.7824 - val_loss: 2.4373 - val_acc: 0.6010\n",
            "Epoch 59/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.2511 - acc: 0.7858 - val_loss: 2.4280 - val_acc: 0.6058\n",
            "Epoch 60/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.2391 - acc: 0.7876 - val_loss: 2.4202 - val_acc: 0.6063\n",
            "Epoch 61/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.2261 - acc: 0.7914 - val_loss: 2.4251 - val_acc: 0.6062\n",
            "Epoch 62/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.2154 - acc: 0.7930 - val_loss: 2.4198 - val_acc: 0.6065\n",
            "Epoch 63/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.2033 - acc: 0.7951 - val_loss: 2.4190 - val_acc: 0.6073\n",
            "Epoch 64/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1942 - acc: 0.7975 - val_loss: 2.4182 - val_acc: 0.6087\n",
            "Epoch 65/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 1.1821 - acc: 0.8000 - val_loss: 2.4274 - val_acc: 0.6050\n",
            "Epoch 66/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1727 - acc: 0.8028 - val_loss: 2.4281 - val_acc: 0.6079\n",
            "Epoch 67/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.1622 - acc: 0.8045 - val_loss: 2.4125 - val_acc: 0.6101\n",
            "Epoch 68/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1522 - acc: 0.8058 - val_loss: 2.4162 - val_acc: 0.6091\n",
            "Epoch 69/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1440 - acc: 0.8089 - val_loss: 2.4110 - val_acc: 0.6109\n",
            "Epoch 70/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 1.1330 - acc: 0.8107 - val_loss: 2.4227 - val_acc: 0.6104\n",
            "Epoch 71/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1235 - acc: 0.8123 - val_loss: 2.4178 - val_acc: 0.6111\n",
            "Epoch 72/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.1153 - acc: 0.8143 - val_loss: 2.4188 - val_acc: 0.6111\n",
            "Epoch 73/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.1062 - acc: 0.8161 - val_loss: 2.4166 - val_acc: 0.6110\n",
            "Epoch 74/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0973 - acc: 0.8181 - val_loss: 2.4223 - val_acc: 0.6105\n",
            "Epoch 75/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 1.0883 - acc: 0.8198 - val_loss: 2.4224 - val_acc: 0.6112\n",
            "Epoch 76/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0807 - acc: 0.8213 - val_loss: 2.4218 - val_acc: 0.6112\n",
            "Epoch 77/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 1.0727 - acc: 0.8230 - val_loss: 2.4186 - val_acc: 0.6130\n",
            "Epoch 78/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0631 - acc: 0.8249 - val_loss: 2.4193 - val_acc: 0.6126\n",
            "Epoch 79/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.0559 - acc: 0.8264 - val_loss: 2.4275 - val_acc: 0.6126\n",
            "Epoch 80/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 1.0474 - acc: 0.8280 - val_loss: 2.4291 - val_acc: 0.6106\n",
            "Epoch 81/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0399 - acc: 0.8294 - val_loss: 2.4159 - val_acc: 0.6155\n",
            "Epoch 82/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.0334 - acc: 0.8310 - val_loss: 2.4143 - val_acc: 0.6137\n",
            "Epoch 83/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 1.0252 - acc: 0.8322 - val_loss: 2.4292 - val_acc: 0.6114\n",
            "Epoch 84/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0185 - acc: 0.8340 - val_loss: 2.4326 - val_acc: 0.6116\n",
            "Epoch 85/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 1.0116 - acc: 0.8349 - val_loss: 2.4279 - val_acc: 0.6118\n",
            "Epoch 86/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 1.0041 - acc: 0.8363 - val_loss: 2.4429 - val_acc: 0.6118\n",
            "Epoch 87/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.9978 - acc: 0.8377 - val_loss: 2.4338 - val_acc: 0.6138\n",
            "Epoch 88/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9900 - acc: 0.8394 - val_loss: 2.4235 - val_acc: 0.6144\n",
            "Epoch 89/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9854 - acc: 0.8400 - val_loss: 2.4257 - val_acc: 0.6147\n",
            "Epoch 90/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.9786 - acc: 0.8419 - val_loss: 2.4377 - val_acc: 0.6132\n",
            "Epoch 91/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.9715 - acc: 0.8429 - val_loss: 2.4267 - val_acc: 0.6150\n",
            "Epoch 92/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.9658 - acc: 0.8440 - val_loss: 2.4464 - val_acc: 0.6127\n",
            "Epoch 93/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.9603 - acc: 0.8452 - val_loss: 2.4442 - val_acc: 0.6144\n",
            "Epoch 94/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.9538 - acc: 0.8468 - val_loss: 2.4407 - val_acc: 0.6139\n",
            "Epoch 95/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.9491 - acc: 0.8473 - val_loss: 2.4381 - val_acc: 0.6151\n",
            "Epoch 96/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.9419 - acc: 0.8486 - val_loss: 2.4415 - val_acc: 0.6141\n",
            "Epoch 97/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.9372 - acc: 0.8495 - val_loss: 2.4376 - val_acc: 0.6144\n",
            "Epoch 98/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.9323 - acc: 0.8508 - val_loss: 2.4491 - val_acc: 0.6137\n",
            "Epoch 99/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9277 - acc: 0.8509 - val_loss: 2.4483 - val_acc: 0.6132\n",
            "Epoch 100/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9210 - acc: 0.8527 - val_loss: 2.4608 - val_acc: 0.6117\n",
            "Epoch 101/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9164 - acc: 0.8534 - val_loss: 2.4542 - val_acc: 0.6124\n",
            "Epoch 102/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.9112 - acc: 0.8547 - val_loss: 2.4620 - val_acc: 0.6122\n",
            "Epoch 103/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.9070 - acc: 0.8551 - val_loss: 2.4503 - val_acc: 0.6167\n",
            "Epoch 104/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.9013 - acc: 0.8570 - val_loss: 2.4521 - val_acc: 0.6144\n",
            "Epoch 105/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.8966 - acc: 0.8576 - val_loss: 2.4600 - val_acc: 0.6143\n",
            "Epoch 106/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8921 - acc: 0.8582 - val_loss: 2.4580 - val_acc: 0.6160\n",
            "Epoch 107/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.8865 - acc: 0.8591 - val_loss: 2.4633 - val_acc: 0.6167\n",
            "Epoch 108/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.8834 - acc: 0.8599 - val_loss: 2.4519 - val_acc: 0.6190\n",
            "Epoch 109/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.8777 - acc: 0.8613 - val_loss: 2.4610 - val_acc: 0.6188\n",
            "Epoch 110/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.8742 - acc: 0.8615 - val_loss: 2.4579 - val_acc: 0.6170\n",
            "Epoch 111/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8706 - acc: 0.8623 - val_loss: 2.4634 - val_acc: 0.6159\n",
            "Epoch 112/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.8654 - acc: 0.8632 - val_loss: 2.4647 - val_acc: 0.6182\n",
            "Epoch 113/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8609 - acc: 0.8646 - val_loss: 2.4619 - val_acc: 0.6171\n",
            "Epoch 114/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.8583 - acc: 0.8648 - val_loss: 2.4709 - val_acc: 0.6162\n",
            "Epoch 115/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.8537 - acc: 0.8649 - val_loss: 2.4666 - val_acc: 0.6157\n",
            "Epoch 116/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.8501 - acc: 0.8662 - val_loss: 2.4759 - val_acc: 0.6154\n",
            "Epoch 117/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.8471 - acc: 0.8668 - val_loss: 2.4818 - val_acc: 0.6154\n",
            "Epoch 118/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8425 - acc: 0.8675 - val_loss: 2.4807 - val_acc: 0.6151\n",
            "Epoch 119/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.8387 - acc: 0.8688 - val_loss: 2.4815 - val_acc: 0.6139\n",
            "Epoch 120/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.8364 - acc: 0.8689 - val_loss: 2.4856 - val_acc: 0.6147\n",
            "Epoch 121/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.8323 - acc: 0.8693 - val_loss: 2.4945 - val_acc: 0.6139\n",
            "Epoch 122/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.8282 - acc: 0.8709 - val_loss: 2.4995 - val_acc: 0.6113\n",
            "Epoch 123/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.8251 - acc: 0.8713 - val_loss: 2.4929 - val_acc: 0.6133\n",
            "Epoch 124/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.8213 - acc: 0.8718 - val_loss: 2.4918 - val_acc: 0.6151\n",
            "Epoch 125/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.8179 - acc: 0.8723 - val_loss: 2.4954 - val_acc: 0.6151\n",
            "Epoch 126/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.8151 - acc: 0.8728 - val_loss: 2.4907 - val_acc: 0.6150\n",
            "Epoch 127/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8133 - acc: 0.8738 - val_loss: 2.4956 - val_acc: 0.6133\n",
            "Epoch 128/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.8088 - acc: 0.8737 - val_loss: 2.5018 - val_acc: 0.6132\n",
            "Epoch 129/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.8053 - acc: 0.8748 - val_loss: 2.5005 - val_acc: 0.6157\n",
            "Epoch 130/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.8013 - acc: 0.8757 - val_loss: 2.5038 - val_acc: 0.6128\n",
            "Epoch 131/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.8001 - acc: 0.8762 - val_loss: 2.5257 - val_acc: 0.6099\n",
            "Epoch 132/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7977 - acc: 0.8764 - val_loss: 2.5157 - val_acc: 0.6120\n",
            "Epoch 133/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7944 - acc: 0.8769 - val_loss: 2.5028 - val_acc: 0.6148\n",
            "Epoch 134/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7925 - acc: 0.8776 - val_loss: 2.5064 - val_acc: 0.6118\n",
            "Epoch 135/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.7894 - acc: 0.8780 - val_loss: 2.5144 - val_acc: 0.6131\n",
            "Epoch 136/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.7861 - acc: 0.8789 - val_loss: 2.5164 - val_acc: 0.6108\n",
            "Epoch 137/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.7829 - acc: 0.8795 - val_loss: 2.5236 - val_acc: 0.6124\n",
            "Epoch 138/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.7804 - acc: 0.8796 - val_loss: 2.5151 - val_acc: 0.6121\n",
            "Epoch 139/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.7779 - acc: 0.8803 - val_loss: 2.5232 - val_acc: 0.6117\n",
            "Epoch 140/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.7749 - acc: 0.8809 - val_loss: 2.5286 - val_acc: 0.6126\n",
            "Epoch 141/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.7738 - acc: 0.8810 - val_loss: 2.5326 - val_acc: 0.6118\n",
            "Epoch 142/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.7705 - acc: 0.8817 - val_loss: 2.5187 - val_acc: 0.6142\n",
            "Epoch 143/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7687 - acc: 0.8821 - val_loss: 2.5274 - val_acc: 0.6138\n",
            "Epoch 144/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.7659 - acc: 0.8823 - val_loss: 2.5333 - val_acc: 0.6135\n",
            "Epoch 145/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7637 - acc: 0.8830 - val_loss: 2.5352 - val_acc: 0.6151\n",
            "Epoch 146/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.7614 - acc: 0.8837 - val_loss: 2.5470 - val_acc: 0.6111\n",
            "Epoch 147/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7602 - acc: 0.8837 - val_loss: 2.5370 - val_acc: 0.6132\n",
            "Epoch 148/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7562 - acc: 0.8848 - val_loss: 2.5365 - val_acc: 0.6133\n",
            "Epoch 149/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.7542 - acc: 0.8850 - val_loss: 2.5418 - val_acc: 0.6155\n",
            "Epoch 150/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.7527 - acc: 0.8852 - val_loss: 2.5464 - val_acc: 0.6128\n",
            "Epoch 151/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7492 - acc: 0.8859 - val_loss: 2.5485 - val_acc: 0.6123\n",
            "Epoch 152/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.7484 - acc: 0.8860 - val_loss: 2.5511 - val_acc: 0.6112\n",
            "Epoch 153/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7464 - acc: 0.8868 - val_loss: 2.5609 - val_acc: 0.6117\n",
            "Epoch 154/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.7429 - acc: 0.8871 - val_loss: 2.5618 - val_acc: 0.6110\n",
            "Epoch 155/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.7418 - acc: 0.8876 - val_loss: 2.5642 - val_acc: 0.6134\n",
            "Epoch 156/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.7388 - acc: 0.8884 - val_loss: 2.5596 - val_acc: 0.6117\n",
            "Epoch 157/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7385 - acc: 0.8879 - val_loss: 2.5626 - val_acc: 0.6124\n",
            "Epoch 158/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.7358 - acc: 0.8881 - val_loss: 2.5669 - val_acc: 0.6129\n",
            "Epoch 159/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.7341 - acc: 0.8892 - val_loss: 2.5797 - val_acc: 0.6126\n",
            "Epoch 160/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.7322 - acc: 0.8897 - val_loss: 2.5752 - val_acc: 0.6104\n",
            "Epoch 161/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.7306 - acc: 0.8891 - val_loss: 2.5809 - val_acc: 0.6100\n",
            "Epoch 162/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.7275 - acc: 0.8903 - val_loss: 2.5864 - val_acc: 0.6124\n",
            "Epoch 163/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.7250 - acc: 0.8908 - val_loss: 2.5844 - val_acc: 0.6104\n",
            "Epoch 164/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7258 - acc: 0.8898 - val_loss: 2.5879 - val_acc: 0.6111\n",
            "Epoch 165/420\n",
            "251/251 [==============================] - 68s 273ms/step - loss: 0.7232 - acc: 0.8911 - val_loss: 2.5950 - val_acc: 0.6096\n",
            "Epoch 166/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.7206 - acc: 0.8914 - val_loss: 2.6046 - val_acc: 0.6075\n",
            "Epoch 167/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.7182 - acc: 0.8919 - val_loss: 2.6016 - val_acc: 0.6087\n",
            "Epoch 168/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.7171 - acc: 0.8920 - val_loss: 2.6051 - val_acc: 0.6084\n",
            "Epoch 169/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 0.7164 - acc: 0.8923 - val_loss: 2.5968 - val_acc: 0.6096\n",
            "Epoch 170/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.7151 - acc: 0.8925 - val_loss: 2.6015 - val_acc: 0.6095\n",
            "Epoch 171/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.7131 - acc: 0.8931 - val_loss: 2.6127 - val_acc: 0.6085\n",
            "Epoch 172/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.7110 - acc: 0.8934 - val_loss: 2.6058 - val_acc: 0.6105\n",
            "Epoch 173/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.7094 - acc: 0.8937 - val_loss: 2.6076 - val_acc: 0.6088\n",
            "Epoch 174/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.7082 - acc: 0.8943 - val_loss: 2.6039 - val_acc: 0.6109\n",
            "Epoch 175/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.7065 - acc: 0.8939 - val_loss: 2.6029 - val_acc: 0.6100\n",
            "Epoch 176/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.7053 - acc: 0.8942 - val_loss: 2.6221 - val_acc: 0.6090\n",
            "Epoch 177/420\n",
            "251/251 [==============================] - 68s 273ms/step - loss: 0.7038 - acc: 0.8947 - val_loss: 2.6141 - val_acc: 0.6104\n",
            "Epoch 178/420\n",
            "251/251 [==============================] - 69s 273ms/step - loss: 0.7030 - acc: 0.8950 - val_loss: 2.6158 - val_acc: 0.6098\n",
            "Epoch 179/420\n",
            "251/251 [==============================] - 68s 269ms/step - loss: 0.7006 - acc: 0.8955 - val_loss: 2.6223 - val_acc: 0.6086\n",
            "Epoch 180/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6988 - acc: 0.8955 - val_loss: 2.6166 - val_acc: 0.6108\n",
            "Epoch 181/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.6977 - acc: 0.8961 - val_loss: 2.6223 - val_acc: 0.6099\n",
            "Epoch 182/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.6963 - acc: 0.8961 - val_loss: 2.6217 - val_acc: 0.6109\n",
            "Epoch 183/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.6934 - acc: 0.8969 - val_loss: 2.6232 - val_acc: 0.6099\n",
            "Epoch 184/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6934 - acc: 0.8971 - val_loss: 2.6273 - val_acc: 0.6087\n",
            "Epoch 185/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6916 - acc: 0.8974 - val_loss: 2.6313 - val_acc: 0.6103\n",
            "Epoch 186/420\n",
            "251/251 [==============================] - 69s 273ms/step - loss: 0.6914 - acc: 0.8972 - val_loss: 2.6383 - val_acc: 0.6081\n",
            "Epoch 187/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6889 - acc: 0.8983 - val_loss: 2.6499 - val_acc: 0.6061\n",
            "Epoch 188/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6868 - acc: 0.8985 - val_loss: 2.6324 - val_acc: 0.6102\n",
            "Epoch 189/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6867 - acc: 0.8985 - val_loss: 2.6464 - val_acc: 0.6086\n",
            "Epoch 190/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6861 - acc: 0.8984 - val_loss: 2.6498 - val_acc: 0.6075\n",
            "Epoch 191/420\n",
            "251/251 [==============================] - 69s 275ms/step - loss: 0.6830 - acc: 0.8984 - val_loss: 2.6456 - val_acc: 0.6086\n",
            "Epoch 192/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.6831 - acc: 0.8992 - val_loss: 2.6559 - val_acc: 0.6081\n",
            "Epoch 193/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6824 - acc: 0.8988 - val_loss: 2.6543 - val_acc: 0.6080\n",
            "Epoch 194/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6805 - acc: 0.8998 - val_loss: 2.6532 - val_acc: 0.6104\n",
            "Epoch 195/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.6791 - acc: 0.9000 - val_loss: 2.6608 - val_acc: 0.6047\n",
            "Epoch 196/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6774 - acc: 0.9004 - val_loss: 2.6623 - val_acc: 0.6079\n",
            "Epoch 197/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6775 - acc: 0.9001 - val_loss: 2.6623 - val_acc: 0.6063\n",
            "Epoch 198/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6756 - acc: 0.9004 - val_loss: 2.6534 - val_acc: 0.6079\n",
            "Epoch 199/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6746 - acc: 0.9006 - val_loss: 2.6752 - val_acc: 0.6055\n",
            "Epoch 200/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6729 - acc: 0.9009 - val_loss: 2.6770 - val_acc: 0.6049\n",
            "Epoch 201/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6728 - acc: 0.9009 - val_loss: 2.6800 - val_acc: 0.6063\n",
            "Epoch 202/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.6711 - acc: 0.9013 - val_loss: 2.6777 - val_acc: 0.6061\n",
            "Epoch 203/420\n",
            "251/251 [==============================] - 65s 261ms/step - loss: 0.6698 - acc: 0.9017 - val_loss: 2.6832 - val_acc: 0.6053\n",
            "Epoch 204/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6698 - acc: 0.9016 - val_loss: 2.6920 - val_acc: 0.6024\n",
            "Epoch 205/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6687 - acc: 0.9018 - val_loss: 2.6796 - val_acc: 0.6051\n",
            "Epoch 206/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6666 - acc: 0.9028 - val_loss: 2.6908 - val_acc: 0.6037\n",
            "Epoch 207/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6655 - acc: 0.9028 - val_loss: 2.6798 - val_acc: 0.6052\n",
            "Epoch 208/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6642 - acc: 0.9027 - val_loss: 2.6857 - val_acc: 0.6044\n",
            "Epoch 209/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.6641 - acc: 0.9026 - val_loss: 2.6904 - val_acc: 0.6036\n",
            "Epoch 210/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6619 - acc: 0.9036 - val_loss: 2.6887 - val_acc: 0.6061\n",
            "Epoch 211/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6618 - acc: 0.9034 - val_loss: 2.7113 - val_acc: 0.6008\n",
            "Epoch 212/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6601 - acc: 0.9036 - val_loss: 2.7034 - val_acc: 0.6049\n",
            "Epoch 213/420\n",
            "251/251 [==============================] - 68s 273ms/step - loss: 0.6596 - acc: 0.9039 - val_loss: 2.6799 - val_acc: 0.6064\n",
            "Epoch 214/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6580 - acc: 0.9043 - val_loss: 2.6958 - val_acc: 0.6042\n",
            "Epoch 215/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6580 - acc: 0.9043 - val_loss: 2.6934 - val_acc: 0.6030\n",
            "Epoch 216/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6568 - acc: 0.9042 - val_loss: 2.6917 - val_acc: 0.6041\n",
            "Epoch 217/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.6552 - acc: 0.9048 - val_loss: 2.6866 - val_acc: 0.6059\n",
            "Epoch 218/420\n",
            "251/251 [==============================] - 69s 274ms/step - loss: 0.6555 - acc: 0.9044 - val_loss: 2.7013 - val_acc: 0.6053\n",
            "Epoch 219/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6532 - acc: 0.9050 - val_loss: 2.6975 - val_acc: 0.6066\n",
            "Epoch 220/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.6536 - acc: 0.9049 - val_loss: 2.7081 - val_acc: 0.6044\n",
            "Epoch 221/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6521 - acc: 0.9051 - val_loss: 2.7082 - val_acc: 0.6055\n",
            "Epoch 222/420\n",
            "251/251 [==============================] - 68s 271ms/step - loss: 0.6517 - acc: 0.9054 - val_loss: 2.7092 - val_acc: 0.6048\n",
            "Epoch 223/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.6512 - acc: 0.9050 - val_loss: 2.7158 - val_acc: 0.6043\n",
            "Epoch 224/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6505 - acc: 0.9057 - val_loss: 2.7074 - val_acc: 0.6042\n",
            "Epoch 225/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6489 - acc: 0.9058 - val_loss: 2.7137 - val_acc: 0.6049\n",
            "Epoch 226/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6484 - acc: 0.9062 - val_loss: 2.7193 - val_acc: 0.6050\n",
            "Epoch 227/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6474 - acc: 0.9066 - val_loss: 2.7217 - val_acc: 0.6050\n",
            "Epoch 228/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6453 - acc: 0.9071 - val_loss: 2.7362 - val_acc: 0.6010\n",
            "Epoch 229/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6452 - acc: 0.9067 - val_loss: 2.7315 - val_acc: 0.6029\n",
            "Epoch 230/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6457 - acc: 0.9068 - val_loss: 2.7227 - val_acc: 0.6038\n",
            "Epoch 231/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6431 - acc: 0.9073 - val_loss: 2.7166 - val_acc: 0.6044\n",
            "Epoch 232/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6432 - acc: 0.9072 - val_loss: 2.7319 - val_acc: 0.6032\n",
            "Epoch 233/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6418 - acc: 0.9073 - val_loss: 2.7214 - val_acc: 0.6054\n",
            "Epoch 234/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6424 - acc: 0.9074 - val_loss: 2.7303 - val_acc: 0.6024\n",
            "Epoch 235/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6396 - acc: 0.9077 - val_loss: 2.7373 - val_acc: 0.6030\n",
            "Epoch 236/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.6401 - acc: 0.9076 - val_loss: 2.7284 - val_acc: 0.6027\n",
            "Epoch 237/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6388 - acc: 0.9079 - val_loss: 2.7510 - val_acc: 0.6008\n",
            "Epoch 238/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6381 - acc: 0.9083 - val_loss: 2.7300 - val_acc: 0.6029\n",
            "Epoch 239/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6365 - acc: 0.9087 - val_loss: 2.7478 - val_acc: 0.5995\n",
            "Epoch 240/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6363 - acc: 0.9087 - val_loss: 2.7372 - val_acc: 0.6029\n",
            "Epoch 241/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6356 - acc: 0.9087 - val_loss: 2.7403 - val_acc: 0.6018\n",
            "Epoch 242/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6352 - acc: 0.9088 - val_loss: 2.7338 - val_acc: 0.6049\n",
            "Epoch 243/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6352 - acc: 0.9089 - val_loss: 2.7435 - val_acc: 0.6031\n",
            "Epoch 244/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6332 - acc: 0.9091 - val_loss: 2.7432 - val_acc: 0.6008\n",
            "Epoch 245/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6322 - acc: 0.9097 - val_loss: 2.7425 - val_acc: 0.6022\n",
            "Epoch 246/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6325 - acc: 0.9090 - val_loss: 2.7616 - val_acc: 0.6002\n",
            "Epoch 247/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6325 - acc: 0.9093 - val_loss: 2.7652 - val_acc: 0.5995\n",
            "Epoch 248/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6309 - acc: 0.9100 - val_loss: 2.7642 - val_acc: 0.6001\n",
            "Epoch 249/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6310 - acc: 0.9097 - val_loss: 2.7502 - val_acc: 0.6019\n",
            "Epoch 250/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6286 - acc: 0.9101 - val_loss: 2.7722 - val_acc: 0.6004\n",
            "Epoch 251/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6296 - acc: 0.9100 - val_loss: 2.7564 - val_acc: 0.6018\n",
            "Epoch 252/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6280 - acc: 0.9099 - val_loss: 2.7520 - val_acc: 0.6018\n",
            "Epoch 253/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6274 - acc: 0.9107 - val_loss: 2.7649 - val_acc: 0.5997\n",
            "Epoch 254/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6278 - acc: 0.9101 - val_loss: 2.7625 - val_acc: 0.5988\n",
            "Epoch 255/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6272 - acc: 0.9102 - val_loss: 2.7666 - val_acc: 0.6007\n",
            "Epoch 256/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6259 - acc: 0.9109 - val_loss: 2.7681 - val_acc: 0.6013\n",
            "Epoch 257/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6253 - acc: 0.9106 - val_loss: 2.7568 - val_acc: 0.6017\n",
            "Epoch 258/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6234 - acc: 0.9115 - val_loss: 2.7605 - val_acc: 0.6011\n",
            "Epoch 259/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6233 - acc: 0.9112 - val_loss: 2.7595 - val_acc: 0.5997\n",
            "Epoch 260/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6234 - acc: 0.9111 - val_loss: 2.7781 - val_acc: 0.5968\n",
            "Epoch 261/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6223 - acc: 0.9112 - val_loss: 2.7670 - val_acc: 0.6014\n",
            "Epoch 262/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6220 - acc: 0.9117 - val_loss: 2.7587 - val_acc: 0.6027\n",
            "Epoch 263/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6212 - acc: 0.9115 - val_loss: 2.7707 - val_acc: 0.5995\n",
            "Epoch 264/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6196 - acc: 0.9119 - val_loss: 2.7769 - val_acc: 0.6007\n",
            "Epoch 265/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6202 - acc: 0.9118 - val_loss: 2.7919 - val_acc: 0.5976\n",
            "Epoch 266/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6199 - acc: 0.9123 - val_loss: 2.7822 - val_acc: 0.5991\n",
            "Epoch 267/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6182 - acc: 0.9124 - val_loss: 2.8093 - val_acc: 0.5962\n",
            "Epoch 268/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6182 - acc: 0.9123 - val_loss: 2.7871 - val_acc: 0.5989\n",
            "Epoch 269/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6174 - acc: 0.9121 - val_loss: 2.7990 - val_acc: 0.5970\n",
            "Epoch 270/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6175 - acc: 0.9123 - val_loss: 2.7841 - val_acc: 0.5984\n",
            "Epoch 271/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6160 - acc: 0.9125 - val_loss: 2.7847 - val_acc: 0.5996\n",
            "Epoch 272/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6154 - acc: 0.9130 - val_loss: 2.7821 - val_acc: 0.5999\n",
            "Epoch 273/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6142 - acc: 0.9131 - val_loss: 2.7780 - val_acc: 0.6024\n",
            "Epoch 274/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6143 - acc: 0.9131 - val_loss: 2.7818 - val_acc: 0.6020\n",
            "Epoch 275/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6146 - acc: 0.9128 - val_loss: 2.7941 - val_acc: 0.5992\n",
            "Epoch 276/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6131 - acc: 0.9137 - val_loss: 2.7880 - val_acc: 0.6016\n",
            "Epoch 277/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6125 - acc: 0.9135 - val_loss: 2.7953 - val_acc: 0.5991\n",
            "Epoch 278/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.6134 - acc: 0.9133 - val_loss: 2.7977 - val_acc: 0.5998\n",
            "Epoch 279/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6114 - acc: 0.9141 - val_loss: 2.7983 - val_acc: 0.5991\n",
            "Epoch 280/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6124 - acc: 0.9132 - val_loss: 2.7842 - val_acc: 0.6028\n",
            "Epoch 281/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6103 - acc: 0.9139 - val_loss: 2.7881 - val_acc: 0.6010\n",
            "Epoch 282/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6088 - acc: 0.9140 - val_loss: 2.7967 - val_acc: 0.6003\n",
            "Epoch 283/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6087 - acc: 0.9141 - val_loss: 2.8106 - val_acc: 0.5970\n",
            "Epoch 284/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6092 - acc: 0.9141 - val_loss: 2.7956 - val_acc: 0.5993\n",
            "Epoch 285/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6084 - acc: 0.9143 - val_loss: 2.8065 - val_acc: 0.5992\n",
            "Epoch 286/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6078 - acc: 0.9145 - val_loss: 2.7997 - val_acc: 0.6009\n",
            "Epoch 287/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6076 - acc: 0.9145 - val_loss: 2.8018 - val_acc: 0.6000\n",
            "Epoch 288/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6063 - acc: 0.9145 - val_loss: 2.8037 - val_acc: 0.5991\n",
            "Epoch 289/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6060 - acc: 0.9148 - val_loss: 2.8079 - val_acc: 0.5985\n",
            "Epoch 290/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6061 - acc: 0.9146 - val_loss: 2.7992 - val_acc: 0.6000\n",
            "Epoch 291/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6048 - acc: 0.9148 - val_loss: 2.8140 - val_acc: 0.5982\n",
            "Epoch 292/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6046 - acc: 0.9147 - val_loss: 2.8028 - val_acc: 0.5995\n",
            "Epoch 293/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6038 - acc: 0.9157 - val_loss: 2.8195 - val_acc: 0.5972\n",
            "Epoch 294/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.6038 - acc: 0.9154 - val_loss: 2.8110 - val_acc: 0.5987\n",
            "Epoch 295/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6028 - acc: 0.9153 - val_loss: 2.8239 - val_acc: 0.5961\n",
            "Epoch 296/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6023 - acc: 0.9153 - val_loss: 2.8316 - val_acc: 0.5960\n",
            "Epoch 297/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6024 - acc: 0.9151 - val_loss: 2.8203 - val_acc: 0.5985\n",
            "Epoch 298/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6004 - acc: 0.9155 - val_loss: 2.8295 - val_acc: 0.5977\n",
            "Epoch 299/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.6020 - acc: 0.9156 - val_loss: 2.8216 - val_acc: 0.5975\n",
            "Epoch 300/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.6005 - acc: 0.9159 - val_loss: 2.8312 - val_acc: 0.5975\n",
            "Epoch 301/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.6006 - acc: 0.9158 - val_loss: 2.8314 - val_acc: 0.5971\n",
            "Epoch 302/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.6000 - acc: 0.9158 - val_loss: 2.8403 - val_acc: 0.5954\n",
            "Epoch 303/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5984 - acc: 0.9162 - val_loss: 2.8342 - val_acc: 0.5950\n",
            "Epoch 304/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5990 - acc: 0.9156 - val_loss: 2.8256 - val_acc: 0.5963\n",
            "Epoch 305/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5981 - acc: 0.9165 - val_loss: 2.8332 - val_acc: 0.5977\n",
            "Epoch 306/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5975 - acc: 0.9166 - val_loss: 2.8307 - val_acc: 0.5977\n",
            "Epoch 307/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5972 - acc: 0.9165 - val_loss: 2.8464 - val_acc: 0.5949\n",
            "Epoch 308/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5983 - acc: 0.9164 - val_loss: 2.8359 - val_acc: 0.5983\n",
            "Epoch 309/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5956 - acc: 0.9168 - val_loss: 2.8329 - val_acc: 0.5977\n",
            "Epoch 310/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5973 - acc: 0.9160 - val_loss: 2.8356 - val_acc: 0.5966\n",
            "Epoch 311/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5947 - acc: 0.9168 - val_loss: 2.8451 - val_acc: 0.5969\n",
            "Epoch 312/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5961 - acc: 0.9165 - val_loss: 2.8437 - val_acc: 0.5971\n",
            "Epoch 313/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5949 - acc: 0.9169 - val_loss: 2.8385 - val_acc: 0.5977\n",
            "Epoch 314/420\n",
            "251/251 [==============================] - 68s 273ms/step - loss: 0.5936 - acc: 0.9169 - val_loss: 2.8437 - val_acc: 0.5959\n",
            "Epoch 315/420\n",
            "251/251 [==============================] - 68s 273ms/step - loss: 0.5925 - acc: 0.9174 - val_loss: 2.8480 - val_acc: 0.5974\n",
            "Epoch 316/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.5932 - acc: 0.9173 - val_loss: 2.8482 - val_acc: 0.5947\n",
            "Epoch 317/420\n",
            "251/251 [==============================] - 68s 272ms/step - loss: 0.5930 - acc: 0.9174 - val_loss: 2.8481 - val_acc: 0.5941\n",
            "Epoch 318/420\n",
            "251/251 [==============================] - 69s 275ms/step - loss: 0.5926 - acc: 0.9177 - val_loss: 2.8639 - val_acc: 0.5960\n",
            "Epoch 319/420\n",
            "251/251 [==============================] - 68s 270ms/step - loss: 0.5914 - acc: 0.9179 - val_loss: 2.8512 - val_acc: 0.5964\n",
            "Epoch 320/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5903 - acc: 0.9176 - val_loss: 2.8470 - val_acc: 0.5963\n",
            "Epoch 321/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 0.5926 - acc: 0.9176 - val_loss: 2.8539 - val_acc: 0.5959\n",
            "Epoch 322/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5896 - acc: 0.9180 - val_loss: 2.8498 - val_acc: 0.5967\n",
            "Epoch 323/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5894 - acc: 0.9180 - val_loss: 2.8523 - val_acc: 0.5972\n",
            "Epoch 324/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5914 - acc: 0.9172 - val_loss: 2.8505 - val_acc: 0.5974\n",
            "Epoch 325/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5893 - acc: 0.9182 - val_loss: 2.8481 - val_acc: 0.5965\n",
            "Epoch 326/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5896 - acc: 0.9182 - val_loss: 2.8510 - val_acc: 0.5970\n",
            "Epoch 327/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5893 - acc: 0.9183 - val_loss: 2.8624 - val_acc: 0.5959\n",
            "Epoch 328/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5889 - acc: 0.9178 - val_loss: 2.8661 - val_acc: 0.5949\n",
            "Epoch 329/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5878 - acc: 0.9183 - val_loss: 2.8644 - val_acc: 0.5951\n",
            "Epoch 330/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5882 - acc: 0.9181 - val_loss: 2.8734 - val_acc: 0.5927\n",
            "Epoch 331/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5872 - acc: 0.9182 - val_loss: 2.8749 - val_acc: 0.5953\n",
            "Epoch 332/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5870 - acc: 0.9186 - val_loss: 2.8838 - val_acc: 0.5936\n",
            "Epoch 333/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5860 - acc: 0.9186 - val_loss: 2.8791 - val_acc: 0.5918\n",
            "Epoch 334/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5861 - acc: 0.9188 - val_loss: 2.8681 - val_acc: 0.5936\n",
            "Epoch 335/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5858 - acc: 0.9185 - val_loss: 2.8771 - val_acc: 0.5937\n",
            "Epoch 336/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5856 - acc: 0.9188 - val_loss: 2.8777 - val_acc: 0.5946\n",
            "Epoch 337/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5852 - acc: 0.9185 - val_loss: 2.8849 - val_acc: 0.5927\n",
            "Epoch 338/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5844 - acc: 0.9190 - val_loss: 2.8767 - val_acc: 0.5953\n",
            "Epoch 339/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5847 - acc: 0.9189 - val_loss: 2.8627 - val_acc: 0.5969\n",
            "Epoch 340/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5838 - acc: 0.9187 - val_loss: 2.8772 - val_acc: 0.5964\n",
            "Epoch 341/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5835 - acc: 0.9189 - val_loss: 2.8719 - val_acc: 0.5944\n",
            "Epoch 342/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5839 - acc: 0.9190 - val_loss: 2.8781 - val_acc: 0.5943\n",
            "Epoch 343/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5820 - acc: 0.9196 - val_loss: 2.8769 - val_acc: 0.5952\n",
            "Epoch 344/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5828 - acc: 0.9190 - val_loss: 2.8934 - val_acc: 0.5923\n",
            "Epoch 345/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5826 - acc: 0.9191 - val_loss: 2.8779 - val_acc: 0.5948\n",
            "Epoch 346/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5820 - acc: 0.9192 - val_loss: 2.8774 - val_acc: 0.5943\n",
            "Epoch 347/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5810 - acc: 0.9195 - val_loss: 2.8779 - val_acc: 0.5961\n",
            "Epoch 348/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5815 - acc: 0.9193 - val_loss: 2.8869 - val_acc: 0.5931\n",
            "Epoch 349/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5805 - acc: 0.9195 - val_loss: 2.8807 - val_acc: 0.5957\n",
            "Epoch 350/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5785 - acc: 0.9201 - val_loss: 2.8784 - val_acc: 0.5944\n",
            "Epoch 351/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5797 - acc: 0.9197 - val_loss: 2.8904 - val_acc: 0.5937\n",
            "Epoch 352/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5786 - acc: 0.9201 - val_loss: 2.8874 - val_acc: 0.5929\n",
            "Epoch 353/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5781 - acc: 0.9200 - val_loss: 2.8760 - val_acc: 0.5951\n",
            "Epoch 354/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5789 - acc: 0.9201 - val_loss: 2.8896 - val_acc: 0.5932\n",
            "Epoch 355/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5786 - acc: 0.9201 - val_loss: 2.8718 - val_acc: 0.5956\n",
            "Epoch 356/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5768 - acc: 0.9207 - val_loss: 2.8784 - val_acc: 0.5951\n",
            "Epoch 357/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5777 - acc: 0.9198 - val_loss: 2.8925 - val_acc: 0.5945\n",
            "Epoch 358/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5755 - acc: 0.9212 - val_loss: 2.8872 - val_acc: 0.5941\n",
            "Epoch 359/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5760 - acc: 0.9205 - val_loss: 2.8965 - val_acc: 0.5943\n",
            "Epoch 360/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5769 - acc: 0.9204 - val_loss: 2.8790 - val_acc: 0.5951\n",
            "Epoch 361/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5761 - acc: 0.9205 - val_loss: 2.9046 - val_acc: 0.5933\n",
            "Epoch 362/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5748 - acc: 0.9206 - val_loss: 2.8969 - val_acc: 0.5934\n",
            "Epoch 363/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5760 - acc: 0.9203 - val_loss: 2.8868 - val_acc: 0.5946\n",
            "Epoch 364/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5748 - acc: 0.9208 - val_loss: 2.8923 - val_acc: 0.5959\n",
            "Epoch 365/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.5732 - acc: 0.9217 - val_loss: 2.9000 - val_acc: 0.5925\n",
            "Epoch 366/420\n",
            "251/251 [==============================] - 66s 261ms/step - loss: 0.5754 - acc: 0.9208 - val_loss: 2.9005 - val_acc: 0.5947\n",
            "Epoch 367/420\n",
            "251/251 [==============================] - 66s 262ms/step - loss: 0.5742 - acc: 0.9208 - val_loss: 2.9015 - val_acc: 0.5935\n",
            "Epoch 368/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5731 - acc: 0.9210 - val_loss: 2.9071 - val_acc: 0.5935\n",
            "Epoch 369/420\n",
            "251/251 [==============================] - 66s 263ms/step - loss: 0.5739 - acc: 0.9206 - val_loss: 2.8995 - val_acc: 0.5928\n",
            "Epoch 370/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5739 - acc: 0.9209 - val_loss: 2.8991 - val_acc: 0.5955\n",
            "Epoch 371/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5735 - acc: 0.9209 - val_loss: 2.9128 - val_acc: 0.5942\n",
            "Epoch 372/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5723 - acc: 0.9216 - val_loss: 2.8998 - val_acc: 0.5940\n",
            "Epoch 373/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5719 - acc: 0.9212 - val_loss: 2.8963 - val_acc: 0.5960\n",
            "Epoch 374/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5733 - acc: 0.9210 - val_loss: 2.8991 - val_acc: 0.5975\n",
            "Epoch 375/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5704 - acc: 0.9217 - val_loss: 2.8997 - val_acc: 0.5938\n",
            "Epoch 376/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5707 - acc: 0.9216 - val_loss: 2.9088 - val_acc: 0.5945\n",
            "Epoch 377/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5711 - acc: 0.9213 - val_loss: 2.9172 - val_acc: 0.5915\n",
            "Epoch 378/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5686 - acc: 0.9221 - val_loss: 2.9056 - val_acc: 0.5938\n",
            "Epoch 379/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5697 - acc: 0.9216 - val_loss: 2.9075 - val_acc: 0.5936\n",
            "Epoch 380/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5698 - acc: 0.9215 - val_loss: 2.9117 - val_acc: 0.5935\n",
            "Epoch 381/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5686 - acc: 0.9222 - val_loss: 2.9146 - val_acc: 0.5940\n",
            "Epoch 382/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5685 - acc: 0.9221 - val_loss: 2.9302 - val_acc: 0.5920\n",
            "Epoch 383/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5688 - acc: 0.9217 - val_loss: 2.9186 - val_acc: 0.5930\n",
            "Epoch 384/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5674 - acc: 0.9219 - val_loss: 2.9071 - val_acc: 0.5946\n",
            "Epoch 385/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5675 - acc: 0.9226 - val_loss: 2.9110 - val_acc: 0.5949\n",
            "Epoch 386/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5695 - acc: 0.9218 - val_loss: 2.9041 - val_acc: 0.5947\n",
            "Epoch 387/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5677 - acc: 0.9221 - val_loss: 2.9152 - val_acc: 0.5938\n",
            "Epoch 388/420\n",
            "251/251 [==============================] - 66s 264ms/step - loss: 0.5667 - acc: 0.9224 - val_loss: 2.9107 - val_acc: 0.5954\n",
            "Epoch 389/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5671 - acc: 0.9220 - val_loss: 2.9129 - val_acc: 0.5930\n",
            "Epoch 390/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5652 - acc: 0.9226 - val_loss: 2.9105 - val_acc: 0.5930\n",
            "Epoch 391/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5663 - acc: 0.9225 - val_loss: 2.9209 - val_acc: 0.5936\n",
            "Epoch 392/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5644 - acc: 0.9227 - val_loss: 2.9162 - val_acc: 0.5958\n",
            "Epoch 393/420\n",
            "251/251 [==============================] - 66s 265ms/step - loss: 0.5661 - acc: 0.9222 - val_loss: 2.9178 - val_acc: 0.5939\n",
            "Epoch 394/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5655 - acc: 0.9227 - val_loss: 2.9220 - val_acc: 0.5943\n",
            "Epoch 395/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5647 - acc: 0.9227 - val_loss: 2.9222 - val_acc: 0.5944\n",
            "Epoch 396/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5641 - acc: 0.9228 - val_loss: 2.9309 - val_acc: 0.5936\n",
            "Epoch 397/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5644 - acc: 0.9226 - val_loss: 2.9385 - val_acc: 0.5931\n",
            "Epoch 398/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5655 - acc: 0.9227 - val_loss: 2.9173 - val_acc: 0.5960\n",
            "Epoch 399/420\n",
            "251/251 [==============================] - 67s 265ms/step - loss: 0.5632 - acc: 0.9231 - val_loss: 2.9383 - val_acc: 0.5936\n",
            "Epoch 400/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5632 - acc: 0.9229 - val_loss: 2.9314 - val_acc: 0.5934\n",
            "Epoch 401/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5625 - acc: 0.9233 - val_loss: 2.9379 - val_acc: 0.5936\n",
            "Epoch 402/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5629 - acc: 0.9228 - val_loss: 2.9407 - val_acc: 0.5917\n",
            "Epoch 403/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5616 - acc: 0.9232 - val_loss: 2.9455 - val_acc: 0.5926\n",
            "Epoch 404/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5623 - acc: 0.9230 - val_loss: 2.9457 - val_acc: 0.5931\n",
            "Epoch 405/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5634 - acc: 0.9228 - val_loss: 2.9390 - val_acc: 0.5925\n",
            "Epoch 406/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5621 - acc: 0.9227 - val_loss: 2.9401 - val_acc: 0.5937\n",
            "Epoch 407/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5614 - acc: 0.9229 - val_loss: 2.9478 - val_acc: 0.5910\n",
            "Epoch 408/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5603 - acc: 0.9232 - val_loss: 2.9442 - val_acc: 0.5922\n",
            "Epoch 409/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5615 - acc: 0.9230 - val_loss: 2.9513 - val_acc: 0.5938\n",
            "Epoch 410/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5614 - acc: 0.9232 - val_loss: 2.9562 - val_acc: 0.5915\n",
            "Epoch 411/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5614 - acc: 0.9230 - val_loss: 2.9416 - val_acc: 0.5944\n",
            "Epoch 412/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5605 - acc: 0.9234 - val_loss: 2.9509 - val_acc: 0.5923\n",
            "Epoch 413/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5597 - acc: 0.9236 - val_loss: 2.9572 - val_acc: 0.5917\n",
            "Epoch 414/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5588 - acc: 0.9235 - val_loss: 2.9533 - val_acc: 0.5913\n",
            "Epoch 415/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5593 - acc: 0.9236 - val_loss: 2.9538 - val_acc: 0.5915\n",
            "Epoch 416/420\n",
            "251/251 [==============================] - 67s 266ms/step - loss: 0.5586 - acc: 0.9239 - val_loss: 2.9674 - val_acc: 0.5901\n",
            "Epoch 417/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5594 - acc: 0.9236 - val_loss: 2.9626 - val_acc: 0.5919\n",
            "Epoch 418/420\n",
            "251/251 [==============================] - 67s 269ms/step - loss: 0.5585 - acc: 0.9238 - val_loss: 2.9715 - val_acc: 0.5892\n",
            "Epoch 419/420\n",
            "251/251 [==============================] - 67s 268ms/step - loss: 0.5581 - acc: 0.9238 - val_loss: 2.9693 - val_acc: 0.5903\n",
            "Epoch 420/420\n",
            "251/251 [==============================] - 67s 267ms/step - loss: 0.5576 - acc: 0.9236 - val_loss: 2.9729 - val_acc: 0.5884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4386dc0630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR20wPHWrlKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### saving model\n",
        "model.save_weights('nmt_weights2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDLl9Zk5WjHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ffe854c1-010e-4ca0-892f-191769070ed6"
      },
      "source": [
        "### Ploting epoch vs loss\n",
        "plt.plot(range(len(model.history.history['val_loss'])),model.history.history['loss'])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmPj/z87G91zkRIcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC8nbUUdWjnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ee693e85-9893-4cc5-a0e6-369c07faf16a"
      },
      "source": [
        "### Ploting epoch vs accuracy\n",
        "plt.plot(range(len(model.history.history['acc'])),model.history.history['acc'])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weLsM93KCNEIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xHVFdGRrlKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('nmt_weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqmYe3WHrlKn",
        "colab_type": "text"
      },
      "source": [
        "Inference Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P74csAHurlKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6FG56AKrlKv",
        "colab_type": "text"
      },
      "source": [
        "Decode sample sequeces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEMJMRpNrlKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reymDyptrlON",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itzT7cOPrlOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
        "k=-1\n",
        "res_out=np.array([['the sentence in engish','the sanskrit sentence','predicted translation']])\n",
        "\n",
        "for k in range(1,100):\n",
        "\n",
        "  (input_seq, actual_output), _ = next(val_gen)\n",
        "  decoded_sentence = decode_sequence(input_seq) \n",
        "  res_out=np.append(res_out,np.array([[y_test[k-1:k].values[0][7:-5],X_test[k-1:k].values[0],decoded_sentence[1:-5]]]),axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBIlyweF6COL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame(res_out)\n",
        "df.to_excel('res5.xlsx', encoding='utf16')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLEnsclabjew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afd17bdf-091f-449d-e3ed-1f887035d4fa"
      },
      "source": [
        "res_out"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['the sentence in engish', 'the sanskrit sentence',\n",
              "        'predicted translation'],\n",
              "       ['this is my first day', 'हा माझा पहिलाच दिवस आहे',\n",
              "        'this is a lawyer think'],\n",
              "       ['nagoya is a city famous for its castle',\n",
              "        'नागोया हे शहर आपल्या किल्ल्यासाठी प्रसिद्ध आहे',\n",
              "        'this boy is our very large city'],\n",
              "       ['i dont have a cell phone', 'माझ्याकडे सेलफोन नाहीये',\n",
              "        'i have tv today dont are the'],\n",
              "       ['this boy is lazy', 'हा मुलगा आळशी आहे',\n",
              "        'the son has vegetarian is real'],\n",
              "       ['tom is a musician', 'टॉम संगीतकार आहे', 'tom is is a musician'],\n",
              "       ['whos fasting', 'उपास कोणाचा आहे', ''],\n",
              "       ['what time is your appointment',\n",
              "        'तुमची अपॉइन्टमेन्ट किती वाजताची आहे',\n",
              "        'what time is your get very white with the'],\n",
              "       ['i like him', 'मला तो आवडतो', 'i like that'],\n",
              "       ['i drink milk', 'मी दूध पिते', 'i drink milk'],\n",
              "       ['who did you vote for in the election',\n",
              "        'निवडणुकीत कोणाला मत दिलंत',\n",
              "        'who asked you in are the in this lake'],\n",
              "       ['where is this train going', 'ही ट्रेन कुठे चालली आहे',\n",
              "        'where is this train leaving'],\n",
              "       ['what else do you want to do', 'तुम्हाला अजून काय करायचं आहे',\n",
              "        'what do you want more more'],\n",
              "       ['im gathering information', 'मी माहिती जमा करतोय',\n",
              "        'im there but i need its bad'],\n",
              "       ['i brought you another blanket',\n",
              "        'मी तुमच्यासाठी आणखीन एक चादर आणली',\n",
              "        'i brought more for an blanket'],\n",
              "       ['he got angry with me', 'ते माझ्यावर रागावले', 'he is mad at me'],\n",
              "       ['he has left his family', 'त्याने आपलं कुटुंब सोडलंय',\n",
              "        'he made our students are you'],\n",
              "       ['who made it', 'ते कोणी बनवलं', 'who made that'],\n",
              "       ['watch how i do it', 'मी कसं करते बघा',\n",
              "        'how does me do find tom'],\n",
              "       ['do you have a car', 'तुझ्याकडे गाडी आहे का',\n",
              "        'do you have a car'],\n",
              "       ['he seems to know us', 'तो आपल्याला ओळखतो असं वाटतंय',\n",
              "        'he knows the whole address'],\n",
              "       ['i see them', 'मला ते दिसतात', 'i look for him'],\n",
              "       ['columbus proved that the world is not flat',\n",
              "        'जग सपाट नाही आहे हे कोलंबसने सिद्ध केलं',\n",
              "        'the you killed that he came out of studying it is '],\n",
              "       ['im going to take a bath', 'मी अंघोळ करायला जातेय',\n",
              "        'im going to take a bath'],\n",
              "       ['well bring tom home', 'आम्ही टॉमला घरी आणू',\n",
              "        'let them one start'],\n",
              "       ['i am decorating the classroom', 'मी वर्ग सजवते आहे',\n",
              "        'i am the classroom is a'],\n",
              "       ['he himself said so', 'तेच स्वतः तसं म्हणाले',\n",
              "        'he kept rich did on never this'],\n",
              "       ['do you have a timetable', 'तुझ्याकडे वेळापत्रक आहे का',\n",
              "        'do you have a all man'],\n",
              "       ['she is first in line', 'ती रांगेत पहिली आहे',\n",
              "        'tom is the first in england were popular'],\n",
              "       ['where will you be', 'तुम्ही कुठे असाल', 'youll be today'],\n",
              "       ['he looks suspicious', 'तो संशयास्पद वाटतो', 'he is his bicycle'],\n",
              "       ['i didnt show them anything', 'मी त्यांना काहीही दाखवलं नाही',\n",
              "        'i didnt go kill mary'],\n",
              "       ['we walked to my room', 'आम्ही माझ्या खोलीत चालत गेलो',\n",
              "        'we went into my thief off'],\n",
              "       ['this is our world', 'हे आमचं जग आहे', 'what is a dog'],\n",
              "       ['dont you ever forget that', 'ते तू कधी विसरू नकोस',\n",
              "        'dont forget how three die'],\n",
              "       ['where did you come from', 'तुम्ही कुठून आलात',\n",
              "        'where did you come from'],\n",
              "       ['they say that he is the richest person in the world',\n",
              "        'म्हणतात की तो जगातला सर्वात श्रीमंत व्यक्ती आहे',\n",
              "        'the say that she is of a rich truth she was got'],\n",
              "       ['the majority of japanese temples are made out of wood',\n",
              "        'बहुतेक जपानी देवळं लाकडाची बनलेली असतात',\n",
              "        'most japanese famous else is sons'],\n",
              "       ['tatoebaorg is offline for maintenance',\n",
              "        'tatoebaorg देखरेखीकरिता ऑफलाइन आहे',\n",
              "        'it is for that all says an lake'],\n",
              "       ['her brother goes to school by bus', 'तिचा दादा बसने शाळेत जातो',\n",
              "        'her brother goes to school by bus'],\n",
              "       ['is a prime number', 'एक अविभाज्य संख्या आहे',\n",
              "        'a one little girl are true'],\n",
              "       ['im taking tom home with me', 'मी टॉमला माझ्याबरोबर घरी नेतोय',\n",
              "        'im tom talking'],\n",
              "       ['is tom a canadian i dont know',\n",
              "        'टॉम कॅनेडियन आहे का मला माहीत नाही',\n",
              "        'i know tom didnt do what i did to do it'],\n",
              "       ['he runs', 'तो धावतो', 'he'],\n",
              "       ['i am not going anywhere', 'मी इथेच आहे', 'im not a this'],\n",
              "       ['i dont like his way of talking',\n",
              "        'मला त्याची बोलण्याची पद्धत आवडत नाही',\n",
              "        'i dont like to always with those'],\n",
              "       ['hes wearing a white cotton shirt',\n",
              "        'त्याने एक सफेद कापसाचा शर्ट घातला आहे',\n",
              "        'he is wearing a white white kitchen'],\n",
              "       ['wheres my brother', 'माझा भाऊ कुठेय', 'wheres my brother'],\n",
              "       ['you dont get up as early as your sister',\n",
              "        'तुम्ही आपल्या बहीणीएवढ्या लवकर उठत नाहीत',\n",
              "        'you should get in his hand'],\n",
              "       ['we both live in boston', 'आपण दोघेही बॉस्टनमध्ये राहतो',\n",
              "        'we both live in boston'],\n",
              "       ['tom was right', 'टॉम बरोबर होते', 'tom was right'],\n",
              "       ['theres somebody in there', 'कुणीतरी आहे तिथे',\n",
              "        'there is a cold dont want half'],\n",
              "       ['hes a cardiologist', 'तो कार्डियोलॉजिस्ट आहे',\n",
              "        'he is in a person'],\n",
              "       ['i like to eat korean food', 'मला कोरियन खाणं खायला आवडतं',\n",
              "        'i like eating take names food and yourself'],\n",
              "       ['he earns twenty dollars a day', 'तो दिवसाचे वीस डॉलर कमावतो',\n",
              "        'i sometimes makes me this'],\n",
              "       ['dont you feel anything', 'तुला काहीच वाटत नाही का',\n",
              "        'dont you feel anything'],\n",
              "       ['do you have a motorcycle', 'तुमच्याकडे मोटरसायकल आहे का',\n",
              "        'do you have a motorcycle'],\n",
              "       ['tom got kicked by a horse', 'टॉमला एका घोड्याने लाथ मारली',\n",
              "        'tom one for him very hard'],\n",
              "       ['i was speaking to you', 'मी तुमच्याशी बोलत होतो',\n",
              "        'i was talking to you'],\n",
              "       ['even if it rains hell play golf',\n",
              "        'पाऊस पडला तरीही तो गोल्फ खेळेल',\n",
              "        'if does he sleep up plays the play over'],\n",
              "       ['a card was attached to the gift',\n",
              "        'बक्षीसेला एक कार्ड जोडलं गेलेलं', 'a girl had meet her story'],\n",
              "       ['this is the bar where i drank my first beer',\n",
              "        'मी ज्या बारमध्ये माझी पहिली बियर प्यायलो तो हाच',\n",
              "        'i think of the new this first day does theyre '],\n",
              "       ['i bought tom a hot dog', 'मी टॉमसाठी एक हॉटडॉग विकत घेतला',\n",
              "        'i bought a new for tom'],\n",
              "       ['tom left mary and went to live with another woman',\n",
              "        'टॉम मेरीला सोडून एका दुसर्\\u200dया बाईबरोबर राहायला गेला',\n",
              "        'tom was tom but every one on monday we trust you'],\n",
              "       ['has he failed again', 'तो परत निष्फळ झाला आहे का',\n",
              "        'is he ever enough a letter'],\n",
              "       ['a lion is an animal', 'सिंह हा एक प्राणी आहे',\n",
              "        'this favorite one after canada is the world'],\n",
              "       ['she sells vegetables', 'ती भाज्या विकते', 'they sell look'],\n",
              "       ['tom has already done a lot for us',\n",
              "        'टॉमने आधीच आपल्यासाठी खूप काही केलं आहे',\n",
              "        'tom has has a but mary true'],\n",
              "       ['did you find tom', 'तुम्हाला टॉम सापडला का', 'did you find tom'],\n",
              "       ['when did you arrive', 'तू कधी पोहोचलीस',\n",
              "        'when did you get looked'],\n",
              "       ['they sell fish and meat', 'ते मासे आणि मटण विकतात',\n",
              "        'they sell fish and meat'],\n",
              "       ['did you have breakfast this morning',\n",
              "        'आज सकाळी नाश्ता केलास का', 'did have it breakfast at so many'],\n",
              "       ['he pulled my shirt', 'त्यांनी माझा शर्ट खेचला',\n",
              "        'she pulled my shirt'],\n",
              "       ['everyones lying', 'सगळे खोटं बोलताहेत', 'everyones lying'],\n",
              "       ['what are you scared of', 'तुला कशाची भीती वाटते',\n",
              "        'what are you afraid of'],\n",
              "       ['i dream a lot', 'मला भरपूर स्वप्न पडतात',\n",
              "        'a little beautiful now i'],\n",
              "       ['dont do anything silly', 'मूर्खासारखं काहीतरी करू नका',\n",
              "        'dont do anything else'],\n",
              "       ['president roosevelt won the election of',\n",
              "        'राष्ट्राध्यक्ष रूजवेल्टनी ची निवडणूक जिंकली',\n",
              "        'the children dollars here'],\n",
              "       ['this is three meters long', 'ही तीन मीटर लांब आहे',\n",
              "        'this is three meters long'],\n",
              "       ['you said so yourself', 'असं तूच म्हणालास', 'you said this'],\n",
              "       ['is there a bus stop nearby', 'जवळपास एखादे बस स्थानक आहे का',\n",
              "        'is a bus on today is coming dont called'],\n",
              "       ['why do you think tom wanted to commit suicide',\n",
              "        'तुला काय वाटतं टॉमला आत्महत्या का करायची होती',\n",
              "        'why did you think tom would take his friends'],\n",
              "       ['just ignore tom', 'टॉमला दुर्लक्ष करा', 'give you up here'],\n",
              "       ['tom will never do that', 'टॉम तसं कधीच करणार नाही',\n",
              "        'tom wont never do anything'],\n",
              "       ['how many years has tom lived in boston',\n",
              "        'टॉम बॉस्टनमध्ये किती वर्ष राहिला आहे', 'how many living yet'],\n",
              "       ['he likes ham and eggs', 'त्याला हॅम आणि अंडी आवडतात',\n",
              "        'he likes eggs and are killed'],\n",
              "       ['what will become of us if a war breaks out',\n",
              "        'जर युद्ध सुरू झालं तर आमचं काय होईल',\n",
              "        'if come on what if i say if tom were gone'],\n",
              "       ['he can play a flute', 'तो बासरी वाजवू शकतो',\n",
              "        'he can play a found down'],\n",
              "       ['tom sneezed', 'टॉम शिंकला', 'tom was a few'],\n",
              "       ['the soup is too hot', 'हे सूप खूपच गरम आहे',\n",
              "        'this is very hot isnt something'],\n",
              "       ['if he were here what would he say',\n",
              "        'ते इथे असते तर त्यांनी काय म्हटलं असतं',\n",
              "        'if that was doing that here open tom'],\n",
              "       ['dont take it out on me', 'माझ्यावर राग काढू नका',\n",
              "        'dont take it out on me'],\n",
              "       ['get some rest now', 'आता जरा आराम करा', 'get some rest now'],\n",
              "       ['she doesnt speak to me', 'ती माझ्याशी बोलत नाही',\n",
              "        'she dont talk to me'],\n",
              "       ['i always call my mother on her birthday',\n",
              "        'मी माझ्या आईला तिच्या वाढदिवसाला नेहमीच फोन करते',\n",
              "        'i always call my mother on her house'],\n",
              "       ['its our own fault', 'आपलीच चूक आहे', 'the question is no'],\n",
              "       ['where are the police', 'पोलीस कुठे आहेत',\n",
              "        'where are the police'],\n",
              "       ['tom is making sandwiches', 'टॉम सँडविच बनवतो आहे',\n",
              "        'tom is a my more'],\n",
              "       ['do you have a fever', 'तुला ताप आहे का', 'do you have a fever'],\n",
              "       ['everybody is happy nowadays', 'आजकाल तर सर्वच आनंदी असतात',\n",
              "        'quiet of your train ill be running']], dtype='<U53')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}