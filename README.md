# Computation Linguitsic project : Language tranlator for under-resourced langues using deep learning and NLP
Over the past decade, linguistic computing for under-resourced languages has witnessed significant progress. We currently have numerous models available for well-resourced languages. However, building a natural language translation model for under-resourced language is challenging since huge textual data is the basis of Natural Language Processing. The aim of this project is to explore methods to adapt or transfer word embedding models learnt from well-resourced languages, having extensive corpus available in digital format, to handle related under-resourced languages. Marathi and Sanskrit have been considered for this project. Sanskrit and Marathi are ancient languages with more than 2000-year-old history. However, these languages lack the presence of digital text data. Both, Sanskrit and Marathi, are written in Devanagari script and also considered to be sibling languages. Concepts of data mining, machine learning, data mining, and information visualisation have been applied during the different phases of this project while following CRISP-DM methodology approach which is a standard research methodology for data mining project.

Sanskrit and Marathi translator using NLP Models -  
These models translate selected under-resourced Languages,i.e sanskrit and marathi, to English. This model consists of a sequence-to-sequence model using LSTM. The seq2seq models are usually the best choice for solving complex problems in Natural languge processing (NLP). To use the seq2seq model, an encoder-decoder architecture is needed to be built. The encoder reads the input sequence and summarizes it into a hidden layer of LSTM. The decoder is able to receive this data and generates a sequence based on it.






